# -*- coding: utf-8 -*-
"""DeepExo.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qsKpa9Nr44384teOQBqtegvXP8Da_YwB

#Deep Exo
"""

from torch.utils.data import DataLoader, Dataset
import torchvision.transforms as transforms
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torchvision.utils import make_grid
from torchvision.utils import save_image
#from IPython.display import Image
import matplotlib.pyplot as plt
import numpy as np
import random
import argparse
import os
import sys

#from google.colab import drive
#drive.mount('/content/gdrive')
root_path = root_path = 'DeepExo/'

n_epochs = 50
batch_size_train = 32
batch_size_test = 1000
learning_rate = 0.01
c_ratio = [0.1,0.6]
momentum = 0.5
log_interval = 10

device = torch.device('cuda')

random_seed = 1
torch.backends.cudnn.enabled = False
torch.manual_seed(random_seed)

img_size = 64
data_path = 'data/CNN/'
train_images = np.load(root_path + data_path + 'master_training.npy')
psf_images =  np.load(root_path + data_path + 'tinyPSF.npy')
test_images = np.load(root_path + data_path + 'master_test.npy')
validation_images = np.load(root_path + 'data/GAN/Real_test_confirmed.npy')

print(f"Shape of trainig data: {train_images.shape}")
print(f"Shape of psf data: {psf_images.shape}")
print(f"Shape of test data: {test_images.shape}")
print(f"Shape of validation data: {validation_images.shape}")

def local_normal(data):
        new_imgs_list = []
        for imgs in data:
            local_min = np.min(imgs)
            new_imgs = (imgs - local_min) / np.max(imgs - local_min)
            new_imgs_list.append(new_imgs)
        return np.array(new_imgs_list).reshape(-1, 64, 64)

def crop_center(img,cropx,cropy):
    y,x = img.shape
    startx = x//2 - cropx//2
    starty = y//2 - cropy//2
    return img[starty:starty+cropy, startx:startx+cropx]

def inject_planet(data, psf_library, c_ratio=[0.01, 0.1], x_bound=[4, 61], y_bound=[4, 61], no_blend=False):
        """Inject planet into random location within a frame
        data: single image
        psf_library: collection of libarary (7x7)
        c_ratio: the contrast ratio between max(speckle) and max(psf)*, currently accepting a range
        x_bound: boundary of x position of the injected psf, must be within [0,64-7]
        y_bound: boundary of y position of the injected psf, must be within [0,64-7]
        no_blend: optional flag, used to control whether two psfs can blend into each other or not, default option allows blending.
        """

        image = data.copy()
        pl_num = np.random.randint(1, high=4)
        pos_label = np.zeros([64, 64])
        used_xy = np.array([])
        c_prior = np.linspace(c_ratio[0], c_ratio[1], 100)
        if x_bound[0] < 4 or x_bound[0] > 61:
            raise Exception("current method only injects whole psf")
        if y_bound[0] < 4 or y_bound[0] > 61:
            raise Exception("current method only injects whole psf")

        for num in range(pl_num):
            while True:
                psf_idx = np.random.randint(0, high=psf_library.shape[0])
                Nx = np.random.randint(x_bound[0], high=x_bound[1])
                Ny = np.random.randint(y_bound[0], high=y_bound[1])
                if len(used_xy) == 0:
                    pass
                else:
                    if no_blend:
                        if np.any(np.linalg.norm(np.array([Nx, Ny]) - used_xy) < 3):
                            pass
                    else:
                        if np.any(np.array([Nx, Ny]) == used_xy):
                            pass

                if np.linalg.norm(np.array([Nx, Ny]) - np.array([32.5, 32.5])) < 4:
                    pass
                else:
                    planet_psf=crop_center(psf_library,7,7)
                    brightness_f = c_prior[0] * np.max(image) / np.max(planet_psf)
                    mod = planet_psf * brightness_f
                    image[Ny - 4:Ny + 3, Nx - 4:Nx + 3] += mod
                    used_xy = np.append(used_xy, [Nx, Ny]).reshape(-1, 2)
                    pos_label[Ny - 4:Ny + 3, Nx - 4:Nx + 3] = 1
                    break
        return image, pos_label

def data_preprocess(data,psf_pl,c_ratio,no_blend):

        ## inject planet for train_data
        injected_samples = np.zeros([len(data), 64, 64])
        planet_loc_maps = np.zeros([len(data)*2, 64, 64])
        for i in range(len(data)):
            new_img, loc_map = inject_planet(data[i].reshape(64, 64), psf_pl, c_ratio=c_ratio,no_blend=True)
            injected_samples[i] += new_img
            planet_loc_maps[i] += loc_map

        normalised_injected = local_normal(injected_samples)
        nor_data = local_normal(data)

        dataset = np.zeros([int(len(data) * 2), 64, 64])

        ## Here we normalised each images into [0,1]
        dataset[:len(data)] += normalised_injected
        dataset[len(data):] += nor_data

        label = np.zeros((len(dataset)))
        label[:len(data)] += 1
        print("label size =", label.shape)
        print("data size=", dataset.shape)
        print("number of positive examples", np.sum(label))

        return dataset, label,planet_loc_maps

preprocessed_train, train_label ,_ = data_preprocess(train_images,psf_images,c_ratio=c_ratio,no_blend = True)
preprocessed_test, test_label, loc_maps_test =  data_preprocess(test_images,psf_images,c_ratio=c_ratio,no_blend = True)

class ExoDataset(Dataset):
  def __init__(self,files,labels,transforms=None):
    self.files = files
    self.labels = labels
    self.transforms = transforms

  def __len__(self):
    return(len(self.files))

  def __getitem__(self,index):
    image =np.float32(np.moveaxis(self.files[index],-1,0))
    if self.transforms:
      image = self.transforms(image)
    return image, int(self.labels[index])

transforms = transforms.Compose([
    transforms.ToTensor(),
    transforms.RandomHorizontalFlip(),
    transforms.RandomVerticalFlip(),
    transforms.RandomRotation(np.random.randint(0,359))
]
)

trainset = ExoDataset(preprocessed_train,train_label,transforms=transforms)
testset = ExoDataset(preprocessed_test,test_label,transforms=transforms)
valset = ExoDataset(preprocessed_train,train_label)

dataloader = DataLoader(trainset, batch_size=batch_size_train,shuffle=True)
testloader = DataLoader(testset, batch_size=batch_size_test,shuffle=True)
valloader = DataLoader(valset, batch_size=32,shuffle=True)

examples = enumerate(testloader)
batch_idx, (example_data, example_targets) = next(examples)

class Net(nn.Module):
  def __init__(self):
    super().__init__()


            ###First Convolutional Block
    self.conv1_1 = nn.Conv2d(1,16,kernel_size=3,stride=1,padding=1)
    self.conv1_2 = nn.Conv2d(16,32,kernel_size=3,stride=1,padding=1)
    self.pool1 = nn.MaxPool2d(2,2)
            ###2nd Convolutional block
    self.conv2_1 = nn.Conv2d(32,64,kernel_size=3,stride=1,padding=1)
    self.conv2_2 = nn.Conv2d(64,128,kernel_size=3,stride=1,padding=1)
    self.pool2 = nn.MaxPool2d(2,2)
            ###3rd Convolutional Block
    self.conv3_1 = nn.Conv2d(128,256,kernel_size=3,stride=1,padding=1)
    self.conv3_2 = nn.Conv2d(256,512,kernel_size=3,stride=1,padding=1)
    self.pool3 = nn.MaxPool2d(2,2)

    self.fc1 = nn.Linear(512*8*8,1024)
    self.fc2 = nn.Linear(1024,512)
    self.fc3 = nn.Linear(512,2)

  def forward(self, x):
          x = self.pool1(F.relu(self.conv1_2(F.relu(self.conv1_1(x)))))
          x = self.pool2(F.relu(self.conv2_2(F.relu(self.conv2_1(x)))))
          x = self.pool3(F.relu(self.conv3_2(F.relu(self.conv3_1(x)))))
          x = torch.flatten(x,1)
          x = F.relu(self.fc1(x))
          x = F.relu(self.fc2(x))
          x = self.fc3(x)
          return x


network = Net()
network.to(device)
optimizer = optim.SGD(network.parameters(), lr=learning_rate,
                      momentum=momentum)

train_losses = []
train_counter = []
test_losses = []
test_counter = [i*len(dataloader.dataset) for i in range(n_epochs + 1)]

def train(epoch):
  network.train()
  for batch_idx, (data, target) in enumerate(dataloader):
    data , target = data.to(device),target.to(device)
    optimizer.zero_grad()
    output = network(data)
    loss = F.cross_entropy(output, target)
    loss.backward()
    optimizer.step()
    if batch_idx % log_interval == 0:
      print('Train Epoch: {} [{}/{} ({:.0f}%)]\tLoss: {:.6f}'.format(
        epoch, batch_idx * len(data), len(dataloader.dataset),
        100. * batch_idx / len(dataloader), loss.item()))
      train_losses.append(loss.item())
      train_counter.append(
        (batch_idx*64) + ((epoch-1)*len(dataloader.dataset)))
      torch.save(network.state_dict(), './model.pth')
      torch.save(optimizer.state_dict(), './optimizer.pth')

def test():
  network.eval()
  test_loss = 0
  correct = 0
  with torch.no_grad():
    for data, target in testloader:
      data , target = data.to(device),target.to(device)
      output = network(data)
      test_loss += F.cross_entropy(output, target).item()
      pred = output.data.max(1, keepdim=True)[1]
      correct += pred.eq(target.data.view_as(pred)).sum()
  test_loss /= len(testloader.dataset)
  test_losses.append(test_loss)
  print('\nTest set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\n'.format(
    test_loss, correct, len(testloader.dataset),
    100. * correct / len(testloader.dataset)))

test()
for epoch in range(1, n_epochs + 1):
  train(epoch)
  test()

print(np.shape(test_counter))
print(np.shape(test_losses))

fig = plt.figure()
plt.plot(train_counter, train_losses, color='blue')
plt.scatter(test_counter, test_losses, color='red')
plt.legend(['Train Loss', 'Test Loss'], loc='upper right')
plt.xlabel('number of training examples seen')
plt.ylabel('negative log likelihood loss')
plt.show()

with torch.no_grad():
  output = network(example_data)

fig = plt.figure()
for i in range(6):
  plt.subplot(2,3,i+1)
  plt.tight_layout()
  plt.imshow(example_data[i][0], cmap='gray', interpolation='none')
  plt.title("Prediction: {}".format(
    output.data.max(1, keepdim=True)[1][i].item()))
  plt.xticks([])
  plt.yticks([])
plt.show()
